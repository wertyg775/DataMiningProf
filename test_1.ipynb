{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7874a62-39ae-421f-89fb-5e4405485dbd",
   "metadata": {
    "id": "e7874a62-39ae-421f-89fb-5e4405485dbd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1614428-16ff-435e-8b16-a398d21ea30a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1614428-16ff-435e-8b16-a398d21ea30a",
    "outputId": "f0eb7676-ed3e-4667-a7d0-9da635a233c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\online-retail-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/online-retail-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FU604DAlpy9U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FU604DAlpy9U",
    "outputId": "32a7fedd-244c-4e37-f327-66f27f8d2180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['online_retail_II.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# List files to find the correct CSV name\n",
    "files = os.listdir(path)\n",
    "print(\"Files in directory:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tJnVohJTqWyN",
   "metadata": {
    "id": "tJnVohJTqWyN"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(path + \"/online_retail_II.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85127642-94d6-4621-8e42-1ad25afb7b73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85127642-94d6-4621-8e42-1ad25afb7b73",
    "outputId": "43ac1be1-46af-4e7c-9cd8-e283fd426546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525461, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084408b2-0ec1-4842-acee-160941ad096b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "084408b2-0ec1-4842-acee-160941ad096b",
    "outputId": "cf86520f-ac31-44e1-a7c1-166c6505ee1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.sample(n=5000)\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3290bf-15f2-4b65-bf0e-649dc5cc678c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3290bf-15f2-4b65-bf0e-649dc5cc678c",
    "outputId": "da9f5482-4f2d-410f-e2ae-b8ebef561fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n",
       "       'Price', 'Customer ID', 'Country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2bad9f-d7d0-41da-9499-e2e2be1b07eb",
   "metadata": {
    "id": "fb2bad9f-d7d0-41da-9499-e2e2be1b07eb"
   },
   "outputs": [],
   "source": [
    "df_sample = df_sample.dropna(subset=['Customer ID', 'Price', 'Quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64355cf4-dbce-44e7-860c-84e5a3733847",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64355cf4-dbce-44e7-860c-84e5a3733847",
    "outputId": "10d060f0-70b7-4be5-c091-43b91f738eba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Metadata loaded from file.\n",
      "Trained model loaded from file. Skipping training phase.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python313\\Lib\\site-packages\\sdv\\_utils.py:500: FutureWarning:\n",
      "\n",
      "The 'load' function will be deprecated in future versions of SDV. Please use 'utils.load_synthesizer' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "# Define file paths\n",
    "METADATA_FILE = 'metadata.json'\n",
    "MODEL_FILE = 'my_ctgan_model.pkl'\n",
    "cols_to_model = ['Quantity', 'Price', 'Country']\n",
    "\n",
    "# 1. Handle Metadata\n",
    "if os.path.exists(METADATA_FILE):\n",
    "    metadata = Metadata.load_from_json(METADATA_FILE)\n",
    "    print(\"‚úì Metadata loaded from file.\")\n",
    "else:\n",
    "    # Detect and save if it doesn't exist\n",
    "    df_for_ctgan = df_sample[cols_to_model].copy()\n",
    "    metadata = Metadata.detect_from_dataframe(data=df_for_ctgan, table_name='retail_patterns')\n",
    "    metadata.save_to_json(METADATA_FILE)\n",
    "    print(\"! Metadata detected and saved.\")\n",
    "\n",
    "# 2. Handle the Trained Model\n",
    "if os.path.exists(MODEL_FILE):\n",
    "    # Load the pre-trained synthesizer\n",
    "    synthesizer = CTGANSynthesizer.load(MODEL_FILE)\n",
    "    print(\"Trained model loaded from file. Skipping training phase.\")\n",
    "else:\n",
    "    # Initialize and train if no model is found\n",
    "    print(\"No model found. Starting training (this may take a few minutes)...\")\n",
    "    synthesizer = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        epochs=100,\n",
    "        cuda=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    synthesizer.fit(df_sample[cols_to_model])\n",
    "    # Save the model so you don't have to train again\n",
    "    synthesizer.save(MODEL_FILE)\n",
    "    print(f\"Training complete. Model saved to {MODEL_FILE}.\")\n",
    "\n",
    "# 3. Generate 1,000 numerical/country records\n",
    "# This works instantly once the model is loaded or trained\n",
    "df_numerical_sim = synthesizer.sample(num_rows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a958ea3-47cc-4316-8f14-6be9dd7b43c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numerical_sim.head()\n",
    "len(df_numerical_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ef82dc-8bb4-4620-9393-d3dec04bdfb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35ef82dc-8bb4-4620-9393-d3dec04bdfb1",
    "outputId": "711b910f-338b-4492-daf3-784208b3b018",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing with Gemini 2.5 Flash...\n",
      "Processing batch 1 (rows 0-19)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 2 (rows 20-39)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 3 (rows 40-59)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 4 (rows 60-79)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 5 (rows 80-99)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 6 (rows 100-119)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 7 (rows 120-139)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 8 (rows 140-159)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 9 (rows 160-179)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 10 (rows 180-199)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 11 (rows 200-219)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 12 (rows 220-239)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 13 (rows 240-259)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 14 (rows 260-279)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 15 (rows 280-299)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 16 (rows 300-319)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 17 (rows 320-339)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 18 (rows 340-359)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 19 (rows 360-379)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 20 (rows 380-399)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 21 (rows 400-419)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 22 (rows 420-439)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 23 (rows 440-459)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 24 (rows 460-479)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 25 (rows 480-499)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 26 (rows 500-519)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 27 (rows 520-539)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 28 (rows 540-559)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 29 (rows 560-579)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 30 (rows 580-599)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 31 (rows 600-619)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 32 (rows 620-639)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 33 (rows 640-659)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 34 (rows 660-679)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 35 (rows 680-699)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 36 (rows 700-719)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 37 (rows 720-739)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 38 (rows 740-759)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 39 (rows 760-779)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 40 (rows 780-799)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 41 (rows 800-819)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 42 (rows 820-839)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 43 (rows 840-859)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 44 (rows 860-879)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 45 (rows 880-899)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 46 (rows 900-919)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 47 (rows 920-939)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 48 (rows 940-959)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 49 (rows 960-979)...\n",
      "‚úì Successfully processed 20 records\n",
      "Processing batch 50 (rows 980-999)...\n",
      "‚úì Successfully processed 20 records\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n",
      "Total rows: 1000\n",
      "Successful: 1000\n",
      "Errors: 0\n",
      "\n",
      "Sample output:\n",
      "   Quantity  Price      Country  \\\n",
      "0        11  11.29  Switzerland   \n",
      "1        12   6.50       France   \n",
      "2        16   1.45       Greece   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Premium Arabica Coffee Beans, 11oz pack, rich ...   \n",
      "1               Organic Green Tea Bags, 12-count box   \n",
      "2             Multi-purpose Cleaning Cloths, 16-pack   \n",
      "\n",
      "                                              Review  \n",
      "0  These coffee beans are fantastic! Got 11. Arom...  \n",
      "1  Great organic green tea! Purchased 12 boxes at...  \n",
      "2  These cleaning cloths are a steal! Got 16 for ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "def process_in_batches_gemini(df, batch_size=20):\n",
    "    all_results = []\n",
    "    \n",
    "    # Use Gemini 2.5 Flash - the latest and most capable Flash model\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.5-flash',  # Latest Flash model\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.7,\n",
    "            max_output_tokens=8192,\n",
    "            response_mime_type=\"application/json\"  # Strict JSON mode\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i : i + batch_size]\n",
    "        context_list = [\n",
    "            {\"index\": idx, \"quantity\": row['Quantity'], \"price\": row['Price']} \n",
    "            for idx, (_, row) in enumerate(batch.iterrows())\n",
    "        ]\n",
    "        \n",
    "        # Structured prompt for Gemini 2.5\n",
    "        prompt = f\"\"\"Generate exactly {len(batch)} retail transaction records.\n",
    "\n",
    "Input context: {json.dumps(context_list)}\n",
    "\n",
    "Return a JSON array with this structure:\n",
    "[\n",
    "  {{\"Description\": \"product name and details\", \"Review\": \"customer review\"}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Requirements:\n",
    "- Exactly {len(batch)} objects in the array\n",
    "- Each Description: 30-60 characters\n",
    "- Each Review: 50-100 characters\n",
    "- Make reviews realistic and varied\n",
    "- Base quantity and price mentions on the input context\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing batch {i//batch_size + 1} (rows {i}-{i+len(batch)-1})...\")\n",
    "            \n",
    "            response = model.generate_content(prompt)\n",
    "            content = response.text.strip()\n",
    "            \n",
    "            # Gemini 2.5 with JSON MIME type should return clean JSON\n",
    "            if content.startswith('```'):\n",
    "                content = content.split('```')[1]\n",
    "                if content.startswith('json'):\n",
    "                    content = content[4:]\n",
    "                content = content.strip()\n",
    "            \n",
    "            batch_data = json.loads(content)\n",
    "            \n",
    "            # Handle wrapped responses\n",
    "            if isinstance(batch_data, dict):\n",
    "                if 'transactions' in batch_data:\n",
    "                    batch_data = batch_data['transactions']\n",
    "                elif 'records' in batch_data:\n",
    "                    batch_data = batch_data['records']\n",
    "                else:\n",
    "                    # Get first list value\n",
    "                    for value in batch_data.values():\n",
    "                        if isinstance(value, list):\n",
    "                            batch_data = value\n",
    "                            break\n",
    "            \n",
    "            # Validate count\n",
    "            if len(batch_data) != len(batch):\n",
    "                print(f\"‚ö† Warning: Expected {len(batch)}, got {len(batch_data)} records\")\n",
    "                # Pad or trim\n",
    "                while len(batch_data) < len(batch):\n",
    "                    batch_data.append({\n",
    "                        'Description': 'Generated placeholder', \n",
    "                        'Review': 'Additional record needed'\n",
    "                    })\n",
    "                batch_data = batch_data[:len(batch)]\n",
    "            \n",
    "            all_results.extend(batch_data)\n",
    "            print(f\"‚úì Successfully processed {len(batch_data)} records\")\n",
    "            \n",
    "            # Gemini 2.5 Flash has improved rate limits\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚úó JSON parsing error in batch {i}: {e}\")\n",
    "            print(f\"Response length: {len(content)} chars\")\n",
    "            print(f\"First 300 chars: {content[:300]}\")\n",
    "            print(f\"Last 300 chars: {content[-300:]}\")\n",
    "            \n",
    "            # Try to salvage\n",
    "            try:\n",
    "                last_complete = content.rfind('}')\n",
    "                if last_complete > 0:\n",
    "                    fixed_content = content[:last_complete + 1] + ']'\n",
    "                    batch_data = json.loads(fixed_content)\n",
    "                    \n",
    "                    if isinstance(batch_data, dict):\n",
    "                        batch_data = list(batch_data.values())[0] if batch_data.values() else []\n",
    "                    \n",
    "                    recovered = len(batch_data)\n",
    "                    print(f\"  ‚Ü≥ Recovered {recovered}/{len(batch)} records\")\n",
    "                    all_results.extend(batch_data)\n",
    "                    \n",
    "                    # Fill missing\n",
    "                    missing = len(batch) - recovered\n",
    "                    if missing > 0:\n",
    "                        all_results.extend([\n",
    "                            {'Description': 'Recovery incomplete', 'Review': 'Partial data'}\n",
    "                        ] * missing)\n",
    "                else:\n",
    "                    raise ValueError(\"No complete objects found\")\n",
    "                    \n",
    "            except Exception as recovery_error:\n",
    "                print(f\"  ‚Ü≥ Recovery failed: {recovery_error}\")\n",
    "                all_results.extend([\n",
    "                    {'Description': 'Parse error', 'Review': 'JSON incomplete'}\n",
    "                ] * len(batch))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Unexpected error in batch {i}: {type(e).__name__}: {e}\")\n",
    "            all_results.extend([\n",
    "                {'Description': 'Generation error', 'Review': 'Request failed'}\n",
    "            ] * len(batch))\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Process with Gemini 2.5 Flash\n",
    "print(\"Starting batch processing with Gemini 2.5 Flash...\")\n",
    "df_text = process_in_batches_gemini(df_numerical_sim, batch_size=20)\n",
    "\n",
    "# Join results\n",
    "df_final = pd.concat([df_numerical_sim.reset_index(drop=True), df_text], axis=1)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Processing Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total rows: {len(df_final)}\")\n",
    "print(f\"Successful: {len(df_final[~df_final['Description'].str.contains('error|Error|incomplete|Incomplete', case=False, na=False)])}\")\n",
    "print(f\"Errors: {len(df_final[df_final['Description'].str.contains('error|Error|incomplete|Incomplete', case=False, na=False)])}\")\n",
    "print(f\"\\nSample output:\")\n",
    "print(df_final.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cdfad-51d4-40ab-9f22-c1629483399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 3: LLM for Feature EXTRACTION ==========\n",
    "def extract_features_with_llm(df):\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for i in range(0, len(df), 20):\n",
    "        batch = df.iloc[i:i+20]\n",
    "        reviews_data = [\n",
    "            {\"description\": row['Description'], \"review\": row['Review'], \n",
    "             \"quantity\": row['Quantity'], \"price\": row['Price']}\n",
    "            for _, row in batch.iterrows()\n",
    "        ]\n",
    "        \n",
    "        prompt = f\"\"\"Analyze these transactions and extract business features in JSON format:\n",
    "\n",
    "{json.dumps(reviews_data, indent=2)}\n",
    "\n",
    "For EACH transaction, return:\n",
    "{{\n",
    "  \"sentiment\": \"positive/negative/neutral\",\n",
    "  \"sentiment_score\": float (-1 to 1),\n",
    "  \"risk_level\": \"low/medium/high\",\n",
    "  \"customer_segment\": \"budget_conscious/premium/impulse_buyer/return_prone/bulk_buyer\",\n",
    "  \"churn_risk\": \"low/medium/high\",\n",
    "  \"satisfaction_score\": integer (1-5)\n",
    "}}\n",
    "\n",
    "Return array of exactly {len(batch)} objects.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            features = json.loads(response.text)\n",
    "            if isinstance(features, dict):\n",
    "                features = list(features.values())[0]\n",
    "            all_features.extend(features)\n",
    "            print(f\"‚úì Extracted features: batch {i//20 + 1}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error batch {i}: {e}\")\n",
    "            all_features.extend([{\n",
    "                \"sentiment\": \"neutral\", \"sentiment_score\": 0.0,\n",
    "                \"risk_level\": \"medium\", \"customer_segment\": \"unknown\",\n",
    "                \"churn_risk\": \"medium\", \"satisfaction_score\": 3\n",
    "            }] * len(batch))\n",
    "    \n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "print(\"\\nüîç Extracting features with Gemini...\")\n",
    "df_features = extract_features_with_llm(df_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fcf4d-0bb0-4fb7-b550-baafadf08fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_with_text, df_features], axis=1)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ FINAL DATASET\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total records: {len(df_final)}\")\n",
    "print(f\"Total columns: {len(df_final.columns)}\")\n",
    "print(f\"\\nColumns: {list(df_final.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df_final.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2288ce47-9b2e-4b74-976f-7069d2c7062b",
   "metadata": {
    "id": "2288ce47-9b2e-4b74-976f-7069d2c7062b"
   },
   "outputs": [],
   "source": [
    "# 3. Save as the final simulated dataset (Satisfies Deliverable 2)\n",
    "df_final.to_csv(\"simulated_business_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42c2b7-17e2-4187-9984-add136dd5869",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59df8bab-35ec-4c05-88d9-b1c3416c2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data from simulated_business_records.csv...\n",
      "‚úì Loaded 1000 records with columns: ['Quantity', 'Price', 'Country', 'Description', 'Review']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# ========== STEP 3: Load CSV and Extract Features ==========\n",
    "\n",
    "# Load the CSV file from Step 2\n",
    "CSV_FILE = 'simulated_business_records.csv'  # Update with your actual filename\n",
    "\n",
    "print(f\"üìÇ Loading data from {CSV_FILE}...\")\n",
    "df_with_text = pd.read_csv(CSV_FILE)\n",
    "print(f\"‚úì Loaded {len(df_with_text)} records with columns: {list(df_with_text.columns)}\")\n",
    "\n",
    "def extract_features_with_llm(df, batch_size=20):\n",
    "    \"\"\"Extract business features from text using Gemini 2.5 Flash\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            temperature=0.3  # Lower temperature for consistent feature extraction\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    all_features = []\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Prepare data for analysis\n",
    "        reviews_data = [\n",
    "            {\n",
    "                \"id\": idx,\n",
    "                \"description\": row['Description'], \n",
    "                \"review\": row['Review'], \n",
    "                \"quantity\": row['Quantity'], \n",
    "                \"price\": row['Price']\n",
    "            }\n",
    "            for idx, (_, row) in enumerate(batch.iterrows())\n",
    "        ]\n",
    "        \n",
    "        prompt = f\"\"\"Analyze these {len(batch)} business transactions and extract features in JSON format.\n",
    "\n",
    "Transaction Data:\n",
    "{json.dumps(reviews_data, indent=2)}\n",
    "\n",
    "For EACH transaction, extract these business intelligence features:\n",
    "- sentiment: \"positive\", \"negative\", or \"neutral\"\n",
    "- sentiment_score: float between -1.0 (very negative) and 1.0 (very positive)\n",
    "- risk_level: \"low\", \"medium\", or \"high\" (based on return likelihood, complaints, product issues)\n",
    "- customer_segment: \"budget_conscious\", \"premium\", \"impulse_buyer\", \"return_prone\", or \"bulk_buyer\"\n",
    "- churn_risk: \"low\", \"medium\", or \"high\" (likelihood customer won't return)\n",
    "- satisfaction_score: integer from 1 (very unsatisfied) to 5 (very satisfied)\n",
    "\n",
    "Return a JSON array with exactly {len(batch)} objects matching this structure:\n",
    "[\n",
    "  {{\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"sentiment_score\": 0.8,\n",
    "    \"risk_level\": \"low\",\n",
    "    \"customer_segment\": \"premium\",\n",
    "    \"churn_risk\": \"low\",\n",
    "    \"satisfaction_score\": 5\n",
    "  }},\n",
    "  ...\n",
    "]\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(f\"üîç Processing batch {i//batch_size + 1}/{total_batches} (rows {i}-{i+len(batch)-1})...\")\n",
    "            \n",
    "            response = model.generate_content(prompt)\n",
    "            content = response.text.strip()\n",
    "            \n",
    "            # Parse JSON response\n",
    "            features = json.loads(content)\n",
    "            \n",
    "            # Handle if wrapped in object\n",
    "            if isinstance(features, dict):\n",
    "                # Try common wrapper keys\n",
    "                if 'features' in features:\n",
    "                    features = features['features']\n",
    "                elif 'transactions' in features:\n",
    "                    features = features['transactions']\n",
    "                else:\n",
    "                    # Get first list value\n",
    "                    features = list(features.values())[0]\n",
    "            \n",
    "            # Validate count\n",
    "            if len(features) != len(batch):\n",
    "                print(f\"  ‚ö† Warning: Expected {len(batch)}, got {len(features)} features\")\n",
    "                # Pad if needed\n",
    "                while len(features) < len(batch):\n",
    "                    features.append({\n",
    "                        \"sentiment\": \"neutral\",\n",
    "                        \"sentiment_score\": 0.0,\n",
    "                        \"risk_level\": \"medium\",\n",
    "                        \"customer_segment\": \"unknown\",\n",
    "                        \"churn_risk\": \"medium\",\n",
    "                        \"satisfaction_score\": 3\n",
    "                    })\n",
    "                features = features[:len(batch)]\n",
    "            \n",
    "            all_features.extend(features)\n",
    "            print(f\"  ‚úì Extracted {len(features)} feature sets\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  ‚úó JSON parsing error: {e}\")\n",
    "            print(f\"  Response preview: {content[:200] if 'content' in locals() else 'N/A'}\")\n",
    "            # Fallback features\n",
    "            all_features.extend([{\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"sentiment_score\": 0.0,\n",
    "                \"risk_level\": \"medium\",\n",
    "                \"customer_segment\": \"unknown\",\n",
    "                \"churn_risk\": \"medium\",\n",
    "                \"satisfaction_score\": 3\n",
    "            }] * len(batch))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {type(e).__name__}: {e}\")\n",
    "            # Fallback features\n",
    "            all_features.extend([{\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"sentiment_score\": 0.0,\n",
    "                \"risk_level\": \"medium\",\n",
    "                \"customer_segment\": \"unknown\",\n",
    "                \"churn_risk\": \"medium\",\n",
    "                \"satisfaction_score\": 3\n",
    "            }] * len(batch))\n",
    "    \n",
    "    return pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03658d01-a2b2-4e50-b842-aef8e7bb412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ Starting Feature Extraction with Gemini 2.5 Flash\n",
      "============================================================\n",
      "\n",
      "üîç Processing batch 1/50 (rows 0-19)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 2/50 (rows 20-39)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 3/50 (rows 40-59)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 4/50 (rows 60-79)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 5/50 (rows 80-99)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 6/50 (rows 100-119)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 7/50 (rows 120-139)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 8/50 (rows 140-159)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 9/50 (rows 160-179)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 10/50 (rows 180-199)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 11/50 (rows 200-219)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 12/50 (rows 220-239)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 13/50 (rows 240-259)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 14/50 (rows 260-279)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 15/50 (rows 280-299)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 16/50 (rows 300-319)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 17/50 (rows 320-339)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 18/50 (rows 340-359)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 19/50 (rows 360-379)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 20/50 (rows 380-399)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 21/50 (rows 400-419)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 22/50 (rows 420-439)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 23/50 (rows 440-459)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 24/50 (rows 460-479)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 25/50 (rows 480-499)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 26/50 (rows 500-519)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 27/50 (rows 520-539)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 28/50 (rows 540-559)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 29/50 (rows 560-579)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 30/50 (rows 580-599)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 31/50 (rows 600-619)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 32/50 (rows 620-639)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 33/50 (rows 640-659)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 34/50 (rows 660-679)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 35/50 (rows 680-699)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 36/50 (rows 700-719)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 37/50 (rows 720-739)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 38/50 (rows 740-759)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 39/50 (rows 760-779)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 40/50 (rows 780-799)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 41/50 (rows 800-819)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 42/50 (rows 820-839)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 43/50 (rows 840-859)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 44/50 (rows 860-879)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 45/50 (rows 880-899)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 46/50 (rows 900-919)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 47/50 (rows 920-939)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 48/50 (rows 940-959)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 49/50 (rows 960-979)...\n",
      "  ‚úì Extracted 20 feature sets\n",
      "üîç Processing batch 50/50 (rows 980-999)...\n",
      "  ‚úì Extracted 20 feature sets\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ Starting Feature Extraction with Gemini 2.5 Flash\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df_features = extract_features_with_llm(df_with_text, batch_size=20)\n",
    "\n",
    "# Combine with original data\n",
    "df_final = pd.concat([df_with_text.reset_index(drop=True), df_features.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7d18b0-d40e-4321-b381-0dd8852b2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ FEATURE EXTRACTION COMPLETE\n",
      "============================================================\n",
      "Total records: 1000\n",
      "Total columns: 11\n",
      "\n",
      "Original columns: ['Quantity', 'Price', 'Country', 'Description', 'Review']\n",
      "New feature columns: ['sentiment', 'sentiment_score', 'risk_level', 'customer_segment', 'churn_risk', 'satisfaction_score']\n",
      "\n",
      "üìä Feature Distribution:\n",
      "\n",
      "Sentiment:\n",
      "sentiment\n",
      "positive    877\n",
      "negative     81\n",
      "neutral      42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Customer Segments:\n",
      "customer_segment\n",
      "bulk_buyer          382\n",
      "budget_conscious    264\n",
      "premium             218\n",
      "return_prone         80\n",
      "impulse_buyer        56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Risk Levels:\n",
      "risk_level\n",
      "low       873\n",
      "high       81\n",
      "medium     46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã Sample Records:\n",
      "                                         Description  \\\n",
      "0  Premium Arabica Coffee Beans, 11oz pack, rich ...   \n",
      "1               Organic Green Tea Bags, 12-count box   \n",
      "2             Multi-purpose Cleaning Cloths, 16-pack   \n",
      "\n",
      "                                              Review sentiment  \\\n",
      "0  These coffee beans are fantastic! Got 11. Arom...  positive   \n",
      "1  Great organic green tea! Purchased 12 boxes at...  positive   \n",
      "2  These cleaning cloths are a steal! Got 16 for ...  positive   \n",
      "\n",
      "   sentiment_score  customer_segment risk_level  satisfaction_score  \n",
      "0             0.90        bulk_buyer        low                   5  \n",
      "1             0.85        bulk_buyer        low                   5  \n",
      "2             0.90  budget_conscious        low                   5  \n"
     ]
    }
   ],
   "source": [
    "# ========== Results Summary ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(df_final)}\")\n",
    "print(f\"Total columns: {len(df_final.columns)}\")\n",
    "print(f\"\\nOriginal columns: {list(df_with_text.columns)}\")\n",
    "print(f\"New feature columns: {list(df_features.columns)}\")\n",
    "\n",
    "print(\"\\nüìä Feature Distribution:\")\n",
    "if 'sentiment' in df_features.columns:\n",
    "    print(f\"\\nSentiment:\")\n",
    "    print(df_features['sentiment'].value_counts())\n",
    "    \n",
    "if 'customer_segment' in df_features.columns:\n",
    "    print(f\"\\nCustomer Segments:\")\n",
    "    print(df_features['customer_segment'].value_counts())\n",
    "    \n",
    "if 'risk_level' in df_features.columns:\n",
    "    print(f\"\\nRisk Levels:\")\n",
    "    print(df_features['risk_level'].value_counts())\n",
    "\n",
    "print(\"\\nüìã Sample Records:\")\n",
    "print(df_final[['Description', 'Review', 'sentiment', 'sentiment_score', \n",
    "                'customer_segment', 'risk_level', 'satisfaction_score']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684976c1-4929-4ba6-9ff3-4ced7d890b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Final dataset saved to 'simulated_business_records.csv'\n",
      "üíæ Features only saved to 'extracted_features.csv'\n",
      "\n",
      "‚ú® Done!\n"
     ]
    }
   ],
   "source": [
    "# Save final dataset\n",
    "OUTPUT_FILE = 'simulated_business_records.csv'\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nüíæ Final dataset saved to '{OUTPUT_FILE}'\")\n",
    "\n",
    "# Optional: Save just the features\n",
    "FEATURES_FILE = 'extracted_features.csv'\n",
    "df_features.to_csv(FEATURES_FILE, index=False)\n",
    "print(f\"üíæ Features only saved to '{FEATURES_FILE}'\")\n",
    "\n",
    "print(\"\\n‚ú® Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b588-f79b-46ca-82fc-195d35e4208c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
